---
layout: post
title: "Being Agile"
date: 2013-07-01 20:22
comments: false
categories: 
published: false
---

_"It's not over until the fat lady sings."_

<!-- more -->

[Agile software development](http://en.wikipedia.org/wiki/Agile_software_development)[^1] isn't magic, or a silver bullet, or even a cure-all but rather a set of processes[^2]. But I think overall, it's about being practical and having _discipline_. After a 5+ year journey from waterfall to agile that is my largest takeaway: agile methodology is really about having discipline. Everything else is just metadata.

[^1]: See also: [http://agilemethodology.org/](http://agilemethodology.org/)
[^2]: And maybe even some tooling.

In my previous company we started life as literally eight people in a room. Most all of us were ex-enterprise Java developers from a company that coded and shipped (_on disks_) a J2EE portal server. In our enterprise development days, iteration cycles were six months (short) to a year (normal) and customer feedback was incorporated a year or more later -- if ever. When we started our new company, we decided we would do things differently. We were embarking on a new type of company -- a _web_ start-up and in 2006 we decided we would be _agile_.

####Starting Out With Coconuts.

Our first step was cutting our development and release cycle down from 6+ months to four weeks -- crazy, I know. We were amazed with ourselves and proud that we weren't being all enterprise-y. We would spend a week or so designing, then a week or so implementing, then whatever was left testing and doing QA before we released.

Keep in mind, our release was pushing new code to some servers and restarting Tomcat.

After about a year of this and a particularly painful release over a Christmas/New Year's holiday break where our four-week iteration dragged to more than six weeks not including the holidays, we started to realize that we were not really being agile at all. We frequently would have doubts or questions about what path we were taking, if end users would like _this_ or like _that_ but would convince ourselves to "push through" with whatever we had decided weeks ago as we'd already "come this far" in development. It didn't matter if new information had come to light or new user issues had surfaced. It was slotted as input to the __next__ sprint. 

Our designs and ideas had to be absolutely perfect. To the point where our fighting over features grew more intense and decisions became harder as we added more people. We had grown to twenty-five and with everyone having more or less a direct voice on the product getting stuff done became agonizingly slow. At this point, my boss and mentor and I had a conversation. He relayed to me the origin of the term ["cargo cult"](http://en.wikipedia.org/wiki/Cargo_cult#First_occurrences). If you haven't read an example or know the origin of the phrase, I dare say you are cargo-culting usage of the term. We realized that we were cargo-culting agile development and in fact employing a "mini-waterfall" approach. We had heard fleeting things about agile and XP and thought, well this is what it's like, right? But we weren't seeing any of the supossed results or benefits. Where were our plane gods? Why had they forsaken us? We were slow to react to changes and it was hard to just drop something when we realized that it just wasn't working.

At this point in early 2008 we embarked on some soul-searching and we did something crazy: _we decided to educate ourselves about agile_. We held brown-bag sessions during lunch. We brought in agile speakers. We bought and discussed booksas a company. We _made_ a scrum board over the course of a weekend. We then started to formulate a philosophy that we thought could work for our personalities and our style. Agile means different things to different people but the following is what it ended up meaning to us and what I think fueled us to success over the course of the next 4+ years as a growing engineering organization.

####Fast, Good, & Cheap. How Do You Want it?

![triangle](/images/project_dimension_grid.JPG)

In the world of project management there's a model called the [Triple Constraint](http://en.wikipedia.org/wiki/Project_management_triangle). Any project undertaken is done under a set of three opposing constraints; generally, "Scope", "Cost", and "Time" with the area inside being the derived "Quality" of the project. The idea is that these forces pull in different directions with the common wisdom being that if you fix any two points the other side of the triangle is almost unknowable, i.e., "choose two". TODO: flesh this out

One of the first things we did was half our interation cycle. From four weeks to two. Just to see what would happen. It was a great way to re-focus. With 4 weeks, there was definitely a tendency to focus on minutae for too long and to put off making hard decisions until too late in the cycle. We went through two sprints each at two weeks when we decided to be even more daring and cut our iteration to just one week.

Oh, there was much moaning. "There's no way we can get anything done in a week. It just won't work!" 

The a-ha moment came when we realized that we could shift the question around. It wasn't _"how can we get everything done in one week?"_ It was _"what do we think we can honestly finish in four days of development?"_ 

We would spend the first day in team/group planning sessions followed by four days of development, releasing the fifth day and starting the next sprint's planning session.

/// TODO: REWRITE THIS SECTION

_"the fox knows many things, but the hedgehog knows one big thing"_

Jim Collins introduces _"The Hedgehog Concept"_ in his excellent 2001 book: [_Good To Great: Why Some Companies Make the Leap...and Others Don't_](http://www.jimcollins.com/article_topics/articles/good-to-great.html). The concept is based on an essay by Isaiah Berlin entitled [_"The Hedgehog and the Fox"_](http://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox)[^3].

[^3]: Itself based on a poem by the Ancient Greek poet [Archilochus](http://en.wikipedia.org/wiki/Archilochus).

In Berlin's essay, he subdivides writers and thinkers into two camps: hedgehogs, who see the world based on a single large defining idea and foxes, who draw on a larger set of experiences and for whom the world cannot be reduced to a single idea[^4].

[^4]: See: [The Hedgehog & The Fox](http://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox)

Collins references the essay as a jump off point to introduce the nature of a hedgehog as a preferable metaphore for how "great" companies find success. The concept is modeled as the intersection in Venn diagram of three areas: what you're deeply passionate about, what you can be the **best** in the world at, and finally what drives your economic engine[^5].

[^5]: Basically, what you can do that pays you the most. 

//// TODO: Predictable, reptitive and measurable. And Good to Great, continuing to apply your principles over and over.
#####Consistency and rhythm (i.e., release trains, planes and automobiles)

One key element to solving the constraint problem was imposing some type of consistency and rythm to our iteration cycles in order to   
[Running And A Startup](http://blog.widgetbox.com/blog/2010/11/2/running-and-a-start-up.html)

/// END TODO:

#####Laying the groundwork. Automate all the things.
We realized we seriously needed to automate somethings. We immediately identified places where we thought we needed to be smarter and off-load some of the human interaction. Build & release, and testing.

<a id="build_release"></a>
#####Build & Release.

We moved from a home-made ant build script to [Maven](http://maven.apache.org). If you stick to the 80% that Maven gives you, you'll be golden. Of course, if you venture into trying to do something in the other 20% you'll become extremely frustrated[^6]. At the same time we set up an internal version of the Sonatype [Nexus](http://www.sonatype.org/nexus/) repository.

[^6]: It took us some time to get there but after four years with Maven, we began investigating alternatives. We had some Rubyists amoung us (myself included) and gave a long hard look at [Apache Buildr](http://buildr.apache.org). Sadly, at the critical moment when we were trying to decide if we would move off of Maven, Buildr was adrift as the project fell into seeming unmaintenance. I'm happy to say it is now back in active development but the window was missed and we continued on with Maven.

Additionally (and more complicatedly), we needed to better automate our release process. As our infrastructure and team grew we wanted less people spending less time doing the mundane tasks of copying WAR files out to ever more servers around the world, updating CDN origin servers and flushing caches. We had a lofty goal of an HTML page with two big buttons: <span style="color: green">__RELEASE__</span> and <span style="color: red">__ROLLBACK__</span>[^7] that one could press and watch the magic happen. We didn't \*quite\* get there.

[^7]: The desire for the buttons was a manifestation of our thinking, "Releasing should be as simple as pushing a button. So should a rollback."

We began by re-writing a bunch of hand-rolled shell and perl scripts using a python library called [Fabric](http://docs.fabfile.org/en/1.6/) to handle the tasks involved with "doing a release". Our releases needed to be easily repeatable and ideally incur little to no downtime of any of our services.

In actuality rollbacks were pretty uncommon. Generally, we were able to either a.) live with a bug for a week, or b.) roll-forward through a "dot-release" sometime during the week. Generally though, we had a pretty good handle on what was being released and if it was functioning as expected[^8].

[^8]: See: [Testing](#testing)

As we grew, our server infrastructure grew and changed as well. We went from being only on servers in a dedicated managed hosting environment, to having servers in EC2 east-1 and EC2 west-2 along with Rackspace Cloud. We tried to identify which services needed to be HA (highly-available -- i.e., run in both EC2 east and west simultaneously; generally, these were read-intensive services) as opposed to which could be simply be redundant (run in either EC2 or Rackspace Cloud or our dedicated machines and "failed-over" in case of an incident; typically, these were write-heavy services). This meant we had redundant infrastructure and caches populated in multiple locations but the services were only ever run out of a single location at any given time.

We also eventually introduced Chef into our Operations toolbelt using it to maintain correct versions of different services across a slew of differing machine types.

<a id="testing"></a>
#####Testing.

We focused a big effort on cleaning up existing tests, writing new baseline tests, and implementing a new testing infrastructure. We tried to engender a culture of testing inside of our engineering organization. Working without an official QA department all engineers were held responsible for the quality of their code and the codebase as a whole. Development was not truly "done" until there were tests written (exceptions should be rare) and it was tested. Another boon was setting up a continuous integration (CI) environment: [Hudson](http://jenkins-ci.org/) (and later Jenkins) along with a basic code-quality auditing tool: [Sonar](http://www.sonarsource.org/). 

In doing so, we ended up with three levels of automated testing.

- Unit tests: These ran as part of the Maven build process. These tests typically tested a single unit or piece of code and mocked or stubbed any interaction with any external system or code. The idea is that these tests should run quickly and not make any network calls as they were always being run locally by developers and periodically (SCM was polled every few minutes) in CI.
- Functional/Integration tests: These tests did not run by default in the build process but were instead set up in a Maven profile that was run in CI. Using Hudson|Jenkins we ran these hourly to make sure that no regression had been introduced into the system. These tests included all of our DAO testing which performed CRUD operations against a test MySQL database. As well as Memcached, Redis, and Cassandra tests pointing at test instances of each. And additionally, third-party service testing like sending/receiving data from the various AWS services we used (EMR, RDS, SimpleDB). Our tests would catch when either the tubes were clogged and our connection to various pieces of AWS was interuppted or slow, or when these and other services were having issues.
- WebDriver/[Selenium](http://docs.seleniumhq.org/projects/webdriver/) tests: We originally started with SeleniumIDE, then SeleniumRC, and eventually settled on Google's [WebDriver](http://google-opensource.blogspot.com/2009/05/introducing-webdriver.html) project (which eventually merged with Selenium and became Selenium 2.0). We built out a testing scaffold using the [PageObject](https://code.google.com/p/selenium/wiki/PageObjects) model and developers wrote and maintained these tests in Java.

<a id="scrumy"></a>
#####Scrum Board.

Since we had a remote employee, we gave up on the home-made scrumboard and looked for an internet accessible solution. We happened upon a great little tool called: [Scrumy](http://scrumy.com/). The smallness of their post-it notes made it tough to just shove large stories into a given post-it. Being constrained by readable space forced us to decompose large details into smaller ones. And self-policing made it such that we wouldn't let ourselves get away with a task such as "Figure stuff out".

#####Tasking & Sizing.

In the beginning we played ["Planning Poker"](http://en.wikipedia.org/wiki/Planning_poker) and other sizing/estimation [games ne√© techniques](http://www.infoq.com/articles/agile-estimation-techniques). We quickly gave these up as gimmicky and more of a distraction than useful. Instead we decided to _iterate_ on how we size tasks. Being a group with more relatively senior people than not we decided to jump into having each person or team decompose their features into tasks and try to estimate and size them individually. Every day in scrum we would use part of the time as a group to run an eyeball test over what the task stated and if everyone in the team agreed with the estimated size.

Scrumy's rule-of-thumb on task size is a maximum of 3 hours for a given task. Lots of different tools or processes use different units to measure task size. In my opinion, "hours" are a natural way of thinking about a task. It's easy to conceptualize how much think you think you need to accomplish a goal. We interpreted this as "if you create a task that's larger than 3 hours you've probably not thought through the problem enough". Or you better have a good reason.

<a id="issue_tracking"></a>
####Issue Tracking

Having come from an enterprise software company, many of us were very familiar with extensive and large issue databases. Known issues since the inception of the product or products. Undead issues that move from iteration to iteration for years being dropped and assigned to new people once sucking the life out of a current host. We would have none of this.

An issue tracking system should have a high signal-to-noise ratio. If there's too much noise the database becomes ignored, unmanageable, or both. We took the philosophy that issues take priority over new tasks/work. If an issue proved to be complicated enough it was spun out into new work (either a task, a set of tasks, or even a new user story) and tracked as such thus removing/closing the issue in the issue-tracking system. If issues moved one or more sprints the question of if we truly intended to fix it was raised. The goal was to always have our list of current issues at or near __zero__. The mindset of constantly attacking the issue tracking system meant that when those outside the development organization saw an issue in the system, they __knew__ with absolute certainty that it was being worked on or thought about. Training ourselves to do this was not easy and the benefit did not become truly apparent until we were large enough to have multiple groups inside of the organization such that outside views into a team's issue-tracking became common and appreciated.


#####Lighthouse

Rather than going in for a large all-encompassing tool (like JIRA) we instead chose another small lightweight service called: [Lighthouse](http://lighthouseapp.com) for issue-tracking. This, to us, reinforced the idea that issues were not heavyweight things to carry around like baggage from one sprint to the next. But lightweight priorities that should be dug into and addressed. Thus having a tool that __only__ focused on issue-tracking (but with some pretty cool features to boot) was great.


####Tracking Velocity

####Backlog Management

####Feedback Loop

Speaking to the board. Having stakeholders in scrum.